{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMoJ6xQLUWKn8pyQel2DG0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drew-walkerr/Diss_Detecting_Provider_Bias/blob/main/stigmatizing_labels_lexicon_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing sent2vec in terminal"
      ],
      "metadata": {
        "id": "_pW53BgIPPJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bv8_Wp_MmMh",
        "outputId": "5b20cb13-55fd-4905-c6a8-c582f7bc9b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sent2vec installation"
      ],
      "metadata": {
        "id": "P8nkreiQRPNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On first run\n",
        "# cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1\n",
        "# git clone https://github.com/epfml/sent2vec.git\n",
        "# cd sent2vec\n",
        "# make\n",
        "\n",
        "# On each runtime: \n",
        "#%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/sent2vec\n",
        "# Failed: !pip install numpy==1.18.2, although this is the version listed in sent2vec requirements.txt\n",
        "#!pip install Cython --upgrade\n",
        "# in terminal:\n",
        "#!python setup.py build_ext\n",
        "#!pip install . "
      ],
      "metadata": {
        "id": "cBOGhJ7iPOsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238a69b0-569e-46ed-e518-7508bbb002e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/sent2vec\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.18.2\n",
            "  Downloading numpy-1.18.2.zip (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (0.29.34)\n",
            "Collecting Cython\n",
            "  Using cached Cython-0.29.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 856, in _parseNoCache\n",
            "    tokens = fn(instring, tokens_start, ret_tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 287, in wrapper\n",
            "    def wrapper(*args):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1736, in isEnabledFor\n",
            "    if self.manager.disable >= level:\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/sent2vec\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import Cython\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from collections import defaultdict\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import Levenshtein, re\n",
        "import sys\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.spatial import distance"
      ],
      "metadata": {
        "id": "kSAvW6PDKxze"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/1_Data Prep\n",
        "%ls"
      ],
      "metadata": {
        "id": "hihS408RC0xC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f634742-e3cf-4b79-d37d-3cabc93dbf5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/1_Data Prep\n",
            "1_stigmatizing_labels_word_embeddings.py\n",
            "stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv\n",
            "stigmatizing_labels_lexicon_dev.ipynb\n",
            "trig-vectors-phrase.bin\n",
            "trig-vectors-phrase.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading word2vec model"
      ],
      "metadata": {
        "id": "L0VCU7pt8NWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using BioWordVec, trained on MIMIC-III data: \n",
        "\n",
        "1. Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n",
        "2. Chen Q, Peng Y, Lu Z. BioSentVec: creating sentence embeddings for biomedical texts. The 7th IEEE International Conference on Healthcare Informatics. 2019."
      ],
      "metadata": {
        "id": "ZPRmbiUzLjWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Abeed's 3B Twitter Drug Use Word Embeddings\n",
        "#model = KeyedVectors.load_word2vec_format('trig-vectors-phrase.bin', binary=True, encoding='latin-1')\n",
        "\n",
        "# BioWordVec, trained on MIMIC-III \n",
        "model = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/BioWordVec_PubMed_MIMICIII_d200.vec.bin', binary = True)\n",
        "#model_path = \"/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/BioWordVec_PubMed_MIMICIII_d200.bin\"\n",
        "#model = KeyedVectors.load(model_path)\n",
        "#model = Word2Vec.load(model_path)\n",
        "\n",
        "\n",
        "#model2 = gensim.models.Word2Vec.load('trig-vectors-phrase.bin')\n",
        "\n",
        "#model2 = KeyedVectors.load_word2vec_format('trig-vectors-phrase.txt', binary=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zXyxskB8KrW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stigmatizing Labels and Negative Patient Descriptors \n",
        "* NIDA Words Matter list\n",
        "* Extra words selected by substance use experts previously used for MOUD stigma/bias (Jenn Drew and Abeed)\n",
        "* Negative patient descriptor list "
      ],
      "metadata": {
        "id": "x8in2sZC8SUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# stigma OR bias OR stereotype OR abuser OR stereotype\n",
        "# NIDA: https://www.drugabuse.gov/nidamed-medical-health-professionals/health-professions-education/words-matter-terms-to-use-avoid-when-talking-about-addiction\n",
        "# Words Matter: Words to avoid when talking about addiction:\n",
        "# Included: Addict, User, Abuser, Junkie, Alcoholic, Drunk, Habit, Dirty,\n",
        "# Added in this study: stigma, bias, stereotype, shame, blame (From studies on stigma, bias, and types of stigmatization referenced in literature)\n",
        "# Not included: clean, addicted baby, opioid substitution replacement therapy, medication-assisted treatment, former addict, reformed adict\n",
        "\n",
        "# Negative Patient Descriptors: Documenting Racial Bias in the Electronic Health Record, Sun et al. 2022\n",
        "# \"non-adherent\", \"aggressive\", \"agitated\", \"angry\", \"challenging\", \"combative\", \"non-compliant\", \"confront\", \"non-cooperative\", \"defensive\", \"exaggerate\", \"hysterical\", \"un-pleasant\", \"refuse\", \"resist\"\n",
        "bias_stem_words = [\"addict\",\"user\",\"abuser\",\"junkie\",\"alcoholic\", \"drunk\", \"habit\", \"dirty\", \"stigma\",\"bias\",\"stereotype\",\"shame\",\"blame\",\"nonadherent\", \"aggressive\", \"agitated\", \"angry\", \"challenging\", \"combative\", \"noncompliant\", \"confront\", \"noncooperative\", \"defensive\", \"exaggerate\", \"hysterical\", \"unpleasant\", \"refuse\", \"resist\"]\n",
        "\n",
        "\n",
        "\n",
        "bias_words_df = pd.DataFrame({\n",
        "    'stem_word': bias_stem_words\n",
        "})"
      ],
      "metadata": {
        "id": "0ZoYHIJf8Q0p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "bias_words_df['most_similar_words'] = bias_words_df['stem_word'].apply(model.most_similar)\n",
        "\n",
        "bias_words_df_2 = bias_words_df.explode(\"most_similar_words\", ignore_index=True)\n",
        "bias_words_df_2['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n",
        "# bias_words_df_2[['similar_word','similarity_score']] =\n",
        "words_sep = pd.DataFrame(bias_words_df_2['most_similar_words'].values.tolist())\n",
        "words_sep['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n",
        "bias_words_3 = bias_words_df_2.merge(words_sep, on = 'new_word_id')\n",
        "#bias_words_3['similar_word'], bias_words_3['score'] = bias_words_3[3],bias_words_3[4]\n",
        "\n",
        "bias_words_3= bias_words_3.rename(columns={0: \"similar_word\", 1: \"score\"})\n",
        "bias_words_3[\"Relevant_to_study\"] = \"\"\n",
        "bias_words_3.to_csv(\"stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv\")\n",
        "\n",
        "bias_words_3"
      ],
      "metadata": {
        "id": "6QKm74hL-TG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "66821721-7fa3-4c5d-d2c7-5a425e57048a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    stem_word                most_similar_words  new_word_id similar_word  \\\n",
              "0      addict    (exaddict, 0.8582410216331482)            1     exaddict   \n",
              "1      addict     (addict*, 0.8445701003074646)            2      addict*   \n",
              "2      addict    (deaddict, 0.8363768458366394)            3     deaddict   \n",
              "3      addict  (methaddict, 0.8306370973587036)            4   methaddict   \n",
              "4      addict     (addicts, 0.8289745450019836)            5      addicts   \n",
              "..        ...                               ...          ...          ...   \n",
              "275    resist    (wresists, 0.7534369230270386)          276     wresists   \n",
              "276    resist   (withstand, 0.7353253364562988)          277    withstand   \n",
              "277    resist    (resist's, 0.7309495210647583)          278     resist's   \n",
              "278    resist  (resisting/, 0.7280867695808411)          279   resisting/   \n",
              "279    resist    (resistry, 0.7255228161811829)          280     resistry   \n",
              "\n",
              "        score Relevant_to_study  \n",
              "0    0.858241                    \n",
              "1    0.844570                    \n",
              "2    0.836377                    \n",
              "3    0.830637                    \n",
              "4    0.828975                    \n",
              "..        ...               ...  \n",
              "275  0.753437                    \n",
              "276  0.735325                    \n",
              "277  0.730950                    \n",
              "278  0.728087                    \n",
              "279  0.725523                    \n",
              "\n",
              "[280 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad8ddda2-63e4-4cb9-add0-a55d2029d0f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stem_word</th>\n",
              "      <th>most_similar_words</th>\n",
              "      <th>new_word_id</th>\n",
              "      <th>similar_word</th>\n",
              "      <th>score</th>\n",
              "      <th>Relevant_to_study</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>addict</td>\n",
              "      <td>(exaddict, 0.8582410216331482)</td>\n",
              "      <td>1</td>\n",
              "      <td>exaddict</td>\n",
              "      <td>0.858241</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>addict</td>\n",
              "      <td>(addict*, 0.8445701003074646)</td>\n",
              "      <td>2</td>\n",
              "      <td>addict*</td>\n",
              "      <td>0.844570</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>addict</td>\n",
              "      <td>(deaddict, 0.8363768458366394)</td>\n",
              "      <td>3</td>\n",
              "      <td>deaddict</td>\n",
              "      <td>0.836377</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>addict</td>\n",
              "      <td>(methaddict, 0.8306370973587036)</td>\n",
              "      <td>4</td>\n",
              "      <td>methaddict</td>\n",
              "      <td>0.830637</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>addict</td>\n",
              "      <td>(addicts, 0.8289745450019836)</td>\n",
              "      <td>5</td>\n",
              "      <td>addicts</td>\n",
              "      <td>0.828975</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>resist</td>\n",
              "      <td>(wresists, 0.7534369230270386)</td>\n",
              "      <td>276</td>\n",
              "      <td>wresists</td>\n",
              "      <td>0.753437</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>resist</td>\n",
              "      <td>(withstand, 0.7353253364562988)</td>\n",
              "      <td>277</td>\n",
              "      <td>withstand</td>\n",
              "      <td>0.735325</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>resist</td>\n",
              "      <td>(resist's, 0.7309495210647583)</td>\n",
              "      <td>278</td>\n",
              "      <td>resist's</td>\n",
              "      <td>0.730950</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>resist</td>\n",
              "      <td>(resisting/, 0.7280867695808411)</td>\n",
              "      <td>279</td>\n",
              "      <td>resisting/</td>\n",
              "      <td>0.728087</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>resist</td>\n",
              "      <td>(resistry, 0.7255228161811829)</td>\n",
              "      <td>280</td>\n",
              "      <td>resistry</td>\n",
              "      <td>0.725523</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>280 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad8ddda2-63e4-4cb9-add0-a55d2029d0f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad8ddda2-63e4-4cb9-add0-a55d2029d0f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad8ddda2-63e4-4cb9-add0-a55d2029d0f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following 10 most similar words identified, we'll use  stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv to filter out words deemed irrelevant to study by JL and DW. "
      ],
      "metadata": {
        "id": "sIrhzlVq-Y1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misspelling Generator\n",
        "* "
      ],
      "metadata": {
        "id": "4RATkUDy-Nx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Misspelling Generator\n",
        "\n",
        "\n",
        "\n",
        "def generate_spelling_variants(seedwordlist, word_vectors, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1):\n",
        "    \"\"\"\n",
        "        setting -> 0 = weighted levenshtein ratios\n",
        "                -> 1 = standard levenshtein ratios\n",
        "\n",
        "    :param seedwordlist:            list of words for which spelling variants are to be generated\n",
        "    :param word_vectors:            the word vector model\n",
        "    :param semantic_search_length:  the number of semantically similar terms to include in each iteration\n",
        "    :param levenshtein_threshold:   the threshold for levenshtein ratio\n",
        "\n",
        "    :return: dictionary containing the seedwords as key and all the variants as a list of values\n",
        "\n",
        "    \"\"\"\n",
        "    vars = defaultdict(list)\n",
        "    for seedword in seedwordlist:\n",
        "        #a dictionary to hold all the variants, key: the seedword, value: the list of possible misspellings\n",
        "        #a dynamic list of terms that are still to be expanded\n",
        "        terms_to_expand = []\n",
        "        terms_to_expand.append(seedword)\n",
        "        all_expanded_terms = []\n",
        "        level = 1\n",
        "        while len(terms_to_expand)>0:\n",
        "                t = terms_to_expand.pop(0)\n",
        "                all_expanded_terms.append(t)\n",
        "                try:\n",
        "                    similars = word_vectors.most_similar(t, topn=semantic_search_length)\n",
        "                    for similar in similars:\n",
        "                        similar_term = similar[0]\n",
        "                        if setting == 1:\n",
        "                            seq_score = Levenshtein.ratio(str(similar_term),seedword)\n",
        "                        if setting == 0:\n",
        "                            seq_score = weighted_levenshtein_ratio(str(similar_term), seedword)\n",
        "                        if seq_score>levenshtein_threshold:\n",
        "                            if not re.search(r'\\_',similar_term):\n",
        "                                vars[seedword].append(similar_term)\n",
        "                                if not similar_term in all_expanded_terms and not similar_term in terms_to_expand:\n",
        "                                    terms_to_expand.append(similar_term)\n",
        "                except:\n",
        "                        pass\n",
        "                level+=1\n",
        "        vars[seedword] = list(set(vars[seedword]))\n",
        "    return vars\n",
        "\n",
        "bias_stem_words_round_2 = pd.read_csv(\"word_list_round_2.csv\")\n",
        "bias_stem_words_round_2[\"similar_word\"] = bias_stem_words_round_2[\"similar_word\"].replace(\"_\", \" \", regex = True)\n",
        "\n",
        "bias_expanded_word_list = bias_stem_words_round_2[\"similar_word\"]\n",
        "\n",
        "\n",
        "expanded = generate_spelling_variants(bias_expanded_word_list, model2, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1)\n",
        "\n",
        "df = pd.DataFrame.from_dict(expanded, orient ='index')\n",
        "\n",
        "\n",
        "\n",
        "df.to_csv(\"expanded_misspellings.csv\")"
      ],
      "metadata": {
        "id": "fediWTrj-Mro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github Code"
      ],
      "metadata": {
        "id": "uicKnEZFeAid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"andrew.walker@emory.edu\"\n",
        "!git config --global user.name \"drew-walkerr\"\n",
        "!git add \"stigmatizing_labels_lexicon_dev.ipynb\" \"stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv\"\n"
      ],
      "metadata": {
        "id": "BUnt32YSeC2B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git commit -m \"created working BioWordVec expanded words for stigmatizing labels lexicon\"\n",
        "!git push origin master # https://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@github.com/drew-walkerr/Diss_Detecting_Provider_Bias.git"
      ],
      "metadata": {
        "id": "R6ooORlVeIwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f44ce0-814a-4a64-9a5a-474466165a2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 6 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   ../../Doubt Markers/1_Data Prep/doubt_markers_lexicon_dev.ipynb\u001b[m\n",
            "\t\u001b[31mdeleted:    ../../Scare Quotes/1_Data Prep /scare_quote_data_prep.ipynb\u001b[m\n",
            "\t\u001b[31mmodified:   stigmatizing_labels_lexicon_dev.ipynb\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m../../BioWordVec_PubMed_MIMICIII_d200.bin\u001b[m\n",
            "\t\u001b[31m../../BioWordVec_PubMed_MIMICIII_d200.txt\u001b[m\n",
            "\t\u001b[31m../../BioWordVec_PubMed_MIMICIII_d200.vec.bin\u001b[m\n",
            "\t\u001b[31m../../Dockerfile\u001b[m\n",
            "\t\u001b[31m../../Doubt Markers/1_Data Prep/doubt_words_lexicon_stem_and_similar_round1.csv\u001b[m\n",
            "\t\u001b[31m../../Doubt Markers/1_Data Prep/trig-vectors-phrase.bin\u001b[m\n",
            "\t\u001b[31m../../Scare Quotes/1_Data Prep/\u001b[m\n",
            "\t\u001b[31mtrig-vectors-phrase.bin\u001b[m\n",
            "\t\u001b[31mtrig-vectors-phrase.txt\u001b[m\n",
            "\t\u001b[31m../../sent2vec/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/drew-walkerr/Diss_Detecting_Provider_Bias.git'\n",
            "\u001b[m"
          ]
        }
      ]
    }
  ]
}