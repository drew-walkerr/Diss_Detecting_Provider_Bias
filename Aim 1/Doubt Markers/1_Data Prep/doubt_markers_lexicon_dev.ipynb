{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dzD8Q0o79PCIha_DnyBfjkc7mM2whLSi","timestamp":1685627544078}],"authorship_tag":"ABX9TyMjw6+4EFxM5ndbguwsGQFt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFX50pdp738q","executionInfo":{"status":"ok","timestamp":1685630009159,"user_tz":240,"elapsed":31283,"user":{"displayName":"Drew Walker","userId":"14045603896542101477"}},"outputId":"59100e67-a07b-42b5-9505-930c1c51f9cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["import numpy\n","import pandas as pd\n","import Cython\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models.keyedvectors import KeyedVectors\n","from collections import defaultdict\n","from gensim.models import Word2Vec, KeyedVectors\n","import Levenshtein, re\n","import sys\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n"]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Doubt Markers/1_Data Prep\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hihS408RC0xC","executionInfo":{"status":"ok","timestamp":1685630044764,"user_tz":240,"elapsed":304,"user":{"displayName":"Drew Walker","userId":"14045603896542101477"}},"outputId":"0962e1c1-6d57-485d-d304-c2d6d47ebeaf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Doubt Markers/1_Data Prep\n","1_doubt_makers_word_embeddings.py  trig-vectors-phrase.bin\n","doubt_markers_lexicon_dev.ipynb\n"]}]},{"cell_type":"markdown","source":["# Loading word2vec model"],"metadata":{"id":"L0VCU7pt8NWu"}},{"cell_type":"code","source":["\n","model = KeyedVectors.load_word2vec_format('trig-vectors-phrase.bin', binary=True, encoding='latin-1')\n","\n","\n"],"metadata":{"id":"_zXyxskB8KrW","executionInfo":{"status":"ok","timestamp":1685630101065,"user_tz":240,"elapsed":40484,"user":{"displayName":"Drew Walker","userId":"14045603896542101477"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Stigmatizing Labels and Negative Patient Descriptors \n","* NIDA Words Matter list\n","* Extra words selected by substance use experts previously used for MOUD stigma/bias (Jenn Drew and Abeed)\n","* Negative patient descriptor list "],"metadata":{"id":"x8in2sZC8SUv"}},{"cell_type":"code","source":["\n","# Doubt Markers/Evidentials identified through:\n","# Beach et al. 2021 Testimonial Injustice : \n","# Judgment words: \"adamant\", \"apparently\", \"claims\",\"insists\", \"states\", \n","# Evidentials: complains, complained, complaining, denies, denied, denying, endorses, endorsed, endorsing, notes, noting , reports, reporting, says, saying, tells, told, telling \n","\n","## List: \n","# Biber and Finegan, 1989 Styles of stance in english \n","## Doubt adverbs: allegedly, apparently, conceivably, ostensibly, perchance, perhaps, possibly, presumably, purportedly, reportedly, reputedly, seemingly, supposedly\n","## Doubt verbs: disbelieve, doubt, suspected, speculated, \n","## Doubt adjectives: alleged, arguable, conceivable, disputable, doubtful, dubious, imaginable, improbable, presumable, questionable, reputed, supposed, uncertain, likely  \n","# Papafragou et al., 2007, Evidentiality in language and cognition\n","\n","doubt_stem_words = [\"adamant\", \"claims\", \"insists\", \"states\", \"allegedly\", \"apparently\", \"conceivably\", \"ostensibly\", \"perchance\", \"perhaps\", \"possibly\", \"presumably\", \"purportedly\", \"reportedly\", \"reputedly\", \"seemingly\", \"supposedly\", \"disbelieve\", \"doubt\", \"suspected\", \"speculated\", \"alleged\", \"arguable\", \"conceivable\", \"disputable\", \"doubtful\", \"dubious\", \"imaginable\", \"improbable\", \"presumable\", \"questionable\", \"reputed\", \"supposed\", \"uncertain\", \"likely\"]\n","\n","\n","\n","bias_words_df = pd.DataFrame({\n","    'stem_word': doubt_stem_words\n","})"],"metadata":{"id":"0ZoYHIJf8Q0p","executionInfo":{"status":"ok","timestamp":1685630132008,"user_tz":240,"elapsed":126,"user":{"displayName":"Drew Walker","userId":"14045603896542101477"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","\n","bias_words_df['most_similar_words'] = bias_words_df['stem_word'].apply(model.most_similar)\n","\n","bias_words_df_2 = bias_words_df.explode(\"most_similar_words\", ignore_index=True)\n","bias_words_df_2['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n","# bias_words_df_2[['similar_word','similarity_score']] =\n","words_sep = pd.DataFrame(bias_words_df_2['most_similar_words'].values.tolist())\n","words_sep['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n","bias_words_3 = bias_words_df_2.merge(words_sep, on = 'new_word_id')\n","#bias_words_3['similar_word'], bias_words_3['score'] = bias_words_3[3],bias_words_3[4]\n","\n","bias_words_3= bias_words_3.rename(columns={0: \"similar_word\", 1: \"score\"})\n","bias_words_3[\"Relevant_to_study\"] = \"\"\n","bias_words_3.to_csv(\"doubt_words_lexicon_stem_and_similar_round1.csv\")\n","\n","bias_words_3"],"metadata":{"id":"6QKm74hL-TG2","colab":{"base_uri":"https://localhost:8080/","height":659},"executionInfo":{"status":"ok","timestamp":1685630195484,"user_tz":240,"elapsed":10336,"user":{"displayName":"Drew Walker","userId":"14045603896542101477"}},"outputId":"dcb94a40-79f2-48ee-b332-12c8e3e0e1d2"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    stem_word                                most_similar_words  new_word_id  \\\n","0     adamant                     (adament, 0.7605476379394531)            1   \n","1     adamant                        (timid, 0.760417640209198)            2   \n","2     adamant                      (impish, 0.7475950717926025)            3   \n","3     adamant                        (jolly, 0.736566960811615)            4   \n","4     adamant                   (dreamball, 0.7204171419143677)            5   \n","..        ...                                               ...          ...   \n","345    likely                    (probable, 0.5059902667999268)          346   \n","346    likely                    (unlikely, 0.4867241084575653)          347   \n","347    likely    (eliminations_luke_harper, 0.4614644944667816)          348   \n","348    likely  (eliminations_daniel_bryan, 0.45495110750198364)          349   \n","349    likely     (eliminations_bray_wyatt, 0.4410935342311859)          350   \n","\n","                  similar_word     score Relevant_to_study  \n","0                      adament  0.760548                    \n","1                        timid  0.760418                    \n","2                       impish  0.747595                    \n","3                        jolly  0.736567                    \n","4                    dreamball  0.720417                    \n","..                         ...       ...               ...  \n","345                   probable  0.505990                    \n","346                   unlikely  0.486724                    \n","347   eliminations_luke_harper  0.461464                    \n","348  eliminations_daniel_bryan  0.454951                    \n","349    eliminations_bray_wyatt  0.441094                    \n","\n","[350 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-5044dc24-fb3f-4bd2-8fc5-96c9258e337f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>stem_word</th>\n","      <th>most_similar_words</th>\n","      <th>new_word_id</th>\n","      <th>similar_word</th>\n","      <th>score</th>\n","      <th>Relevant_to_study</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>adamant</td>\n","      <td>(adament, 0.7605476379394531)</td>\n","      <td>1</td>\n","      <td>adament</td>\n","      <td>0.760548</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>adamant</td>\n","      <td>(timid, 0.760417640209198)</td>\n","      <td>2</td>\n","      <td>timid</td>\n","      <td>0.760418</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>adamant</td>\n","      <td>(impish, 0.7475950717926025)</td>\n","      <td>3</td>\n","      <td>impish</td>\n","      <td>0.747595</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>adamant</td>\n","      <td>(jolly, 0.736566960811615)</td>\n","      <td>4</td>\n","      <td>jolly</td>\n","      <td>0.736567</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>adamant</td>\n","      <td>(dreamball, 0.7204171419143677)</td>\n","      <td>5</td>\n","      <td>dreamball</td>\n","      <td>0.720417</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>345</th>\n","      <td>likely</td>\n","      <td>(probable, 0.5059902667999268)</td>\n","      <td>346</td>\n","      <td>probable</td>\n","      <td>0.505990</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>346</th>\n","      <td>likely</td>\n","      <td>(unlikely, 0.4867241084575653)</td>\n","      <td>347</td>\n","      <td>unlikely</td>\n","      <td>0.486724</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>347</th>\n","      <td>likely</td>\n","      <td>(eliminations_luke_harper, 0.4614644944667816)</td>\n","      <td>348</td>\n","      <td>eliminations_luke_harper</td>\n","      <td>0.461464</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>348</th>\n","      <td>likely</td>\n","      <td>(eliminations_daniel_bryan, 0.45495110750198364)</td>\n","      <td>349</td>\n","      <td>eliminations_daniel_bryan</td>\n","      <td>0.454951</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>349</th>\n","      <td>likely</td>\n","      <td>(eliminations_bray_wyatt, 0.4410935342311859)</td>\n","      <td>350</td>\n","      <td>eliminations_bray_wyatt</td>\n","      <td>0.441094</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>350 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5044dc24-fb3f-4bd2-8fc5-96c9258e337f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5044dc24-fb3f-4bd2-8fc5-96c9258e337f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5044dc24-fb3f-4bd2-8fc5-96c9258e337f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Following 10 most similar words identified, we'll use  stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv to filter out words deemed irrelevant to study by JL and DW. "],"metadata":{"id":"sIrhzlVq-Y1Z"}},{"cell_type":"markdown","source":["# Misspelling Generator\n","* "],"metadata":{"id":"4RATkUDy-Nx2"}},{"cell_type":"code","source":["\n","## Misspelling Generator\n","\n","\n","\n","def generate_spelling_variants(seedwordlist, word_vectors, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1):\n","    \"\"\"\n","        setting -> 0 = weighted levenshtein ratios\n","                -> 1 = standard levenshtein ratios\n","\n","    :param seedwordlist:            list of words for which spelling variants are to be generated\n","    :param word_vectors:            the word vector model\n","    :param semantic_search_length:  the number of semantically similar terms to include in each iteration\n","    :param levenshtein_threshold:   the threshold for levenshtein ratio\n","\n","    :return: dictionary containing the seedwords as key and all the variants as a list of values\n","\n","    \"\"\"\n","    vars = defaultdict(list)\n","    for seedword in seedwordlist:\n","        #a dictionary to hold all the variants, key: the seedword, value: the list of possible misspellings\n","        #a dynamic list of terms that are still to be expanded\n","        terms_to_expand = []\n","        terms_to_expand.append(seedword)\n","        all_expanded_terms = []\n","        level = 1\n","        while len(terms_to_expand)>0:\n","                t = terms_to_expand.pop(0)\n","                all_expanded_terms.append(t)\n","                try:\n","                    similars = word_vectors.most_similar(t, topn=semantic_search_length)\n","                    for similar in similars:\n","                        similar_term = similar[0]\n","                        if setting == 1:\n","                            seq_score = Levenshtein.ratio(str(similar_term),seedword)\n","                        if setting == 0:\n","                            seq_score = weighted_levenshtein_ratio(str(similar_term), seedword)\n","                        if seq_score>levenshtein_threshold:\n","                            if not re.search(r'\\_',similar_term):\n","                                vars[seedword].append(similar_term)\n","                                if not similar_term in all_expanded_terms and not similar_term in terms_to_expand:\n","                                    terms_to_expand.append(similar_term)\n","                except:\n","                        pass\n","                level+=1\n","        vars[seedword] = list(set(vars[seedword]))\n","    return vars\n","\n","bias_stem_words_round_2 = pd.read_csv(\"word_list_round_2.csv\")\n","bias_stem_words_round_2[\"similar_word\"] = bias_stem_words_round_2[\"similar_word\"].replace(\"_\", \" \", regex = True)\n","\n","bias_expanded_word_list = bias_stem_words_round_2[\"similar_word\"]\n","\n","\n","expanded = generate_spelling_variants(bias_expanded_word_list, model2, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1)\n","\n","df = pd.DataFrame.from_dict(expanded, orient ='index')\n","\n","\n","\n","df.to_csv(\"expanded_misspellings.csv\")"],"metadata":{"id":"fediWTrj-Mro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add \"doubt_markers_le\""],"metadata":{"id":"kp4Hk33mZpKa"},"execution_count":null,"outputs":[]}]}