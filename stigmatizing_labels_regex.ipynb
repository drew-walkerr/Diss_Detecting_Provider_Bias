{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drew-walkerr/Diss_Detecting_Provider_Bias/blob/main/stigmatizing_labels_regex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e_jJIxYma-D",
        "outputId": "a8403231-63df-4be1-ac32-3d245bdf7578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import scipy\n",
        "import Cython\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb0ttBEhXHwA"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8qUJG9uUH7U",
        "outputId": "6bd2b1dd-145c-4da3-e967-b96341c2948f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/1_Data Prep\n",
            "1_stigmatizing_labels_word_embeddings.py\n",
            "annotation_Annie_sample_stigmatizing_labels.csv\n",
            "annotation_Drew_sample_stigmatizing_labels.csv\n",
            "cocode_sample_stigmatizing_labels.csv\n",
            "Drew_initial_sample_stigmatizing_labels.csv\n",
            "Drew_initial_sample_stigmatizing_labels.gsheet\n",
            "reliability_sample_stigmatizing_labels.csv\n",
            "stigma_expanded_misspellings.csv\n",
            "stigmatizing_labels_dataframe_annotate.csv\n",
            "stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv\n",
            "stigmatizing_labels_lexicon_dev.ipynb\n",
            "stigmatizing_labels_regex.ipynb\n",
            "stigmatizing_word_list_round_1_dw_pruned.csv\n",
            "stigma_word_list_round_2.csv\n",
            "stigma_word_list_round_3_gpt.csv\n",
            "trig-vectors-phrase.bin\n",
            "trig-vectors-phrase.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/1_Data Prep\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYGMRwJgnh7o"
      },
      "source": [
        "BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76ci9q6dnjel"
      },
      "outputs": [],
      "source": [
        "#Replace 'project_id' with your BigQuery project ID\n",
        "\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(project='detecting-bias-mimic-iii')\n",
        "\n",
        "icd_query = ('''SELECT *\n",
        "                FROM physionet-data.mimiciii_clinical.diagnoses_icd\n",
        "                LIMIT 1000''')\n",
        "\n",
        "icd = client.query(icd_query).to_dataframe()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxEZ_dROm-Y6"
      },
      "source": [
        "ICD codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qenQlyfVm5Fu",
        "outputId": "30be8cc6-bca4-4a44-cada-bad0aa2c7c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
            "784  176454       15872   139593        3     73382\n",
            "889  196755       17664   123887        7     33829\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2 entries, 784 to 889\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   ROW_ID      2 non-null      Int64 \n",
            " 1   SUBJECT_ID  2 non-null      Int64 \n",
            " 2   HADM_ID     2 non-null      Int64 \n",
            " 3   SEQ_NUM     2 non-null      Int64 \n",
            " 4   ICD9_CODE   2 non-null      object\n",
            "dtypes: Int64(4), object(1)\n",
            "memory usage: 104.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#Filter for ICD codes for 282.60-282.69, referring to sickle cell types w/wo crisis\n",
        "#2824 for thalassemia w + w/o crisis (282.41-282.42)\n",
        "#SCD: 2826,2824\n",
        "#Chronic Pain 3382\n",
        "#Opioid dependencies: 3040,3047 (combo),\n",
        "#HIV/AIDS ^042$\n",
        "\n",
        "icds_of_interest = icd[icd['ICD9_CODE'].str.contains('2826|2824|3040|3047|3382|^042$', na=False)]\n",
        "# Issues here-- will cause more leading numbers\n",
        "print(icds_of_interest.head())\n",
        "\n",
        "icds_of_interest.info()\n",
        "\n",
        "#patients_unique = icds_of_interest['SUBJECT_ID'].drop_duplicates()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFo4Bhh4m_fl"
      },
      "source": [
        "Notes and patients load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCtUfH54m7FZ",
        "outputId": "30ecaed7-6289-400d-aaaf-4b9a7d2db8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ROW_ID  SUBJECT_ID  HADM_ID   CHARTDATE           CHARTTIME  \\\n",
            "0   183661       11861   110532  2125-09-22                 NaT   \n",
            "1   256904       58417     <NA>  2187-03-12                 NaT   \n",
            "2   312692       11085     <NA>  2157-01-18                 NaT   \n",
            "3   120578         977   128363  2176-01-21                 NaT   \n",
            "4  2016292       25247   135854  2154-03-25 2154-03-25 18:50:00   \n",
            "\n",
            "            STORETIME       CATEGORY DESCRIPTION   CGID  ISERROR  \\\n",
            "0                 NaT            ECG      Report   <NA>     <NA>   \n",
            "1                 NaT            ECG      Report   <NA>     <NA>   \n",
            "2                 NaT            ECG      Report   <NA>     <NA>   \n",
            "3                 NaT            ECG      Report   <NA>     <NA>   \n",
            "4 2154-03-25 18:50:00  Nursing/other      Report  18433     <NA>   \n",
            "\n",
            "                                                TEXT  \n",
            "0  Sinus tachycardia. Compared to the previous tr...  \n",
            "1  Sinus bradycardia.  Since the previous tracing...  \n",
            "2  Sinus tachycardia.  Q waves in leads V1-V1 of ...  \n",
            "3  Sinus rhythm. Consider prior inferior myocardi...  \n",
            "4  1 Term Respiratory Distress\\n2 Infant with Pot...  \n",
            "   ROW_ID  SUBJECT_ID GENDER        DOB        DOD   DOD_HOSP    DOD_SSN  \\\n",
            "0      49          56      F 1804-01-02 2104-01-08 2104-01-08 2104-01-08   \n",
            "1   17808       18848      F 2042-08-21 2128-01-08 2128-01-08 2128-01-08   \n",
            "2   36390       61056      F 2067-04-11 2152-01-08 2152-01-08 2152-01-08   \n",
            "3   25371       26889      F 2115-11-04 2164-01-08 2164-01-08 2164-01-08   \n",
            "4   17328       18333      F 2094-01-21 2168-01-08 2168-01-08 2168-01-08   \n",
            "\n",
            "   EXPIRE_FLAG  \n",
            "0            1  \n",
            "1            1  \n",
            "2            1  \n",
            "3            1  \n",
            "4            1  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2083180 entries, 0 to 2083179\n",
            "Data columns (total 18 columns):\n",
            " #   Column       Dtype         \n",
            "---  ------       -----         \n",
            " 0   ROW_ID_x     Int64         \n",
            " 1   SUBJECT_ID   Int64         \n",
            " 2   HADM_ID      Int64         \n",
            " 3   CHARTDATE    dbdate        \n",
            " 4   CHARTTIME    datetime64[ns]\n",
            " 5   STORETIME    datetime64[ns]\n",
            " 6   CATEGORY     object        \n",
            " 7   DESCRIPTION  object        \n",
            " 8   CGID         Int64         \n",
            " 9   ISERROR      Int64         \n",
            " 10  TEXT         object        \n",
            " 11  ROW_ID_y     Int64         \n",
            " 12  GENDER       object        \n",
            " 13  DOB          datetime64[ns]\n",
            " 14  DOD          datetime64[ns]\n",
            " 15  DOD_HOSP     datetime64[ns]\n",
            " 16  DOD_SSN      datetime64[ns]\n",
            " 17  EXPIRE_FLAG  Int64         \n",
            "dtypes: Int64(7), datetime64[ns](6), dbdate(1), object(4)\n",
            "memory usage: 315.9+ MB\n"
          ]
        }
      ],
      "source": [
        "note_query = ('''SELECT *\n",
        "                FROM physionet-data.mimiciii_notes.noteevents''')\n",
        "                #LIMIT 100000''')\n",
        "\n",
        "NOTES = client.query(note_query).to_dataframe()\n",
        "\n",
        "peek_notes = NOTES.head()\n",
        "print(peek_notes)\n",
        "\n",
        "## Patients\n",
        "\n",
        "patient_query = ('''SELECT *\n",
        "                FROM physionet-data.mimiciii_clinical.patients''')\n",
        "\n",
        "PATIENTS = client.query(patient_query).to_dataframe()\n",
        "\n",
        "peek_patients = PATIENTS.head()\n",
        "print(peek_patients)\n",
        "\n",
        "biased_notes_patients_corpus = NOTES.merge(PATIENTS, on = 'SUBJECT_ID')\n",
        "\n",
        "biased_notes_patients_corpus.info()\n",
        "# Find unique chart types and decide to limit them\n",
        "    # Remove Radiology, ECG, Respiratory, Echo notes\n",
        "biased_notes_patients_corpus_filtered = biased_notes_patients_corpus[biased_notes_patients_corpus[\"CATEGORY\"].str.contains(\"Radiology|ECG|Respiratory|Echo\")==False]\n",
        "# biased_notes_patients_corpus_filtered.to_csv(\"biased_notes_patients_corpus_filtered.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv66agb8nLkm"
      },
      "source": [
        "Explode by sentence, regex quote match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "LTRd7oW_nJwS",
        "outputId": "03a8351a-2311-4959-9454-485cc9dd89be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-639a220d647f>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  full_dataframe[\"Sentence\"] = full_dataframe[\"TEXT\"].apply(lambda x: [sent.text for sent in nlp(x).sents])\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-639a220d647f>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfull_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+|\\\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfull_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/mimic_iii_sentence_tokenized.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_wrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ],
      "source": [
        "full_dataframe = biased_notes_patients_corpus_filtered\n",
        "# Tokenize by sentence\n",
        "nlp = English()  # just the language with no model\n",
        "\n",
        "nlp.add_pipe('sentencizer')\n",
        "full_dataframe[\"Sentence\"] = full_dataframe[\"TEXT\"].apply(lambda x: [sent.text for sent in nlp(x).sents])\n",
        "full_dataframe = full_dataframe.explode(\"Sentence\", ignore_index=True)\n",
        "full_dataframe.rename(columns={\"Unnamed: 0\": \"ROW_ID_new\"}, inplace=True)\n",
        "full_dataframe.index.name = \"Sentence ID\"\n",
        "\n",
        "full_dataframe['Sentence'].replace(r'\\s+|\\\\n', ' ', regex=True, inplace=True)\n",
        "\n",
        "full_dataframe.to_csv(\"/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/mimic_iii_sentence_tokenized.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz9gVz4JTa9-"
      },
      "source": [
        "# Regex matching all stemwords + any semantically similar word identified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-iEUt0X01tv",
        "outputId": "6fa31eaa-4abb-433d-cfbb-453811c5194e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ce85557fbcaa>:3: DtypeWarning: Columns (15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  full_dataframe = pd.read_csv(\"/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/mimic_iii_sentence_tokenized.csv\")\n"
          ]
        }
      ],
      "source": [
        "# Read in sentence tokenized MIMIC-III\n",
        "\n",
        "full_dataframe = pd.read_csv(\"/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/mimic_iii_sentence_tokenized.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0h-YAQ98TRWY",
        "outputId": "3353f716-d74f-4e6c-e04e-3c1455df9803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         stem_word                                       similar_word  \\\n",
            "0           abuser  [ivdabuser, abuser's, abused-abuser, abusers, ...   \n",
            "1        alcoholic  [non-alcoholic/alcoholic, prealcoholic, ex-alc...   \n",
            "2            angry                 [angry/disgusted, angry-disgusted]   \n",
            "3         chat_gpt  [\"hysterical, drug addict, non-compliant, lazy...   \n",
            "4        combative                                      [combatively]   \n",
            "5        defensive  [overdefensive, counterdefensive, defensive/of...   \n",
            "6     drug-seeking  [''drug-seeking, drug-seeking/drug-taking, 'dr...   \n",
            "7            drunk    [drunks, drunken, drunkest, drunker, drunkenly]   \n",
            "8   frequent-flyer  [frequent-flyers, frequent-flier, frequent-fvl...   \n",
            "9       hysterical  [hysteric, hystericals, hysterics, schizo-hyst...   \n",
            "10          junkie                       [junkies, 'junkie, junkie's]   \n",
            "11     nonadherent  [non-adherent, nonadherents, onadherent, nonad...   \n",
            "12    noncompliant  [non-compliant, noncompliants, noncompliant/co...   \n",
            "13  noncooperative  [non-cooperative, noncooperation, noncooperati...   \n",
            "14          refuse                                [refuses, refusing]   \n",
            "15       reluctant  [reluctantly, reluctanly, reluctants, morerelu...   \n",
            "16      unpleasant  [unpleasantries, unpleasant/annoying, unpleasa...   \n",
            "\n",
            "                                                regex  \n",
            "0   abuser|ivdabuser|abuser's|abused\\-abuser|abuse...  \n",
            "1   alcoholic|non\\-alcoholic/alcoholic|prealcoholi...  \n",
            "2              angry|angry/disgusted|angry\\-disgusted  \n",
            "3   chat_gpt|\"hysterical|drug\\ addict|non\\-complia...  \n",
            "4                               combative|combatively  \n",
            "5   defensive|overdefensive|counterdefensive|defen...  \n",
            "6   drug\\-seeking|''drug\\-seeking|drug\\-seeking/dr...  \n",
            "7     drunk|drunks|drunken|drunkest|drunker|drunkenly  \n",
            "8   frequent\\-flyer|frequent\\-flyers|frequent\\-fli...  \n",
            "9   hysterical|hysteric|hystericals|hysterics|schi...  \n",
            "10                    junkie|junkies|'junkie|junkie's  \n",
            "11  nonadherent|non\\-adherent|nonadherents|onadher...  \n",
            "12  noncompliant|non\\-compliant|noncompliants|nonc...  \n",
            "13  noncooperative|non\\-cooperative|noncooperation...  \n",
            "14                            refuse|refuses|refusing  \n",
            "15  reluctant|reluctantly|reluctanly|reluctants|mo...  \n",
            "16  unpleasant|unpleasantries|unpleasant/annoying|...  \n",
            "Size before patient related filter: 20045\n",
            "Final size after patient related filter: 9964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Sentence ID  ROW_ID_x  SUBJECT_ID   HADM_ID   CHARTDATE  ...  \\\n",
              "12929593     12929593     48563       13855  107256.0  2137-05-06  ...   \n",
              "12929712     12929712    371212       13855  107256.0  2137-05-01  ...   \n",
              "12929975     12929975     48561       13855  125990.0  2135-07-06  ...   \n",
              "12930089     12930089     48562       13855  108743.0  2135-12-19  ...   \n",
              "5774027       5774027     22009        5760  169133.0  2109-01-22  ...   \n",
              "...               ...       ...         ...       ...         ...  ...   \n",
              "18023216     18023216   1313078        3895  151195.0  2140-04-03  ...   \n",
              "18060104     18060104    584615       58757  130745.0  2140-06-15  ...   \n",
              "18061939     18061939     52008       58757  130745.0  2140-06-19  ...   \n",
              "18062226     18062226     42197       67418  161948.0  2189-10-12  ...   \n",
              "18259482     18259482   1469313       15904  125754.0  2155-09-13  ...   \n",
              "\n",
              "             DOD_SSN EXPIRE_FLAG  \\\n",
              "12929593         NaN           0   \n",
              "12929712         NaN           0   \n",
              "12929975         NaN           0   \n",
              "12930089         NaN           0   \n",
              "5774027          NaN           0   \n",
              "...              ...         ...   \n",
              "18023216         NaN           1   \n",
              "18060104         NaN           1   \n",
              "18061939         NaN           1   \n",
              "18062226         NaN           0   \n",
              "18259482  2155-10-30           1   \n",
              "\n",
              "                                                   Sentence  \\\n",
              "12929593  Per OMR, reports that his parents were substan...   \n",
              "12929712  Per OMR, reports that his parents were substan...   \n",
              "12929975   Reports that his parents were substance abuse...   \n",
              "12930089  Per OMR, reports that his parents were substan...   \n",
              "5774027   Her hepatic workup was consistent with non-alc...   \n",
              "...                                                     ...   \n",
              "18023216   Lungs diminished at bases, clear bil upper lo...   \n",
              "18060104   # STROKE - s/p R sided CVA in '[**33**] with ...   \n",
              "18061939   # STROKE - s/p R sided CVA in '[**33**] with ...   \n",
              "18062226  He had a full work up at that time, according ...   \n",
              "18259482  was initially unwilling to be turned but is mu...   \n",
              "\n",
              "                                                      regex  \\\n",
              "12929593  abuser|ivdabuser|abuser's|abused\\-abuser|abuse...   \n",
              "12929712  abuser|ivdabuser|abuser's|abused\\-abuser|abuse...   \n",
              "12929975  abuser|ivdabuser|abuser's|abused\\-abuser|abuse...   \n",
              "12930089  abuser|ivdabuser|abuser's|abused\\-abuser|abuse...   \n",
              "5774027   alcoholic|non\\-alcoholic/alcoholic|prealcoholi...   \n",
              "...                                                     ...   \n",
              "18023216  reluctant|reluctantly|reluctanly|reluctants|mo...   \n",
              "18060104  reluctant|reluctantly|reluctanly|reluctants|mo...   \n",
              "18061939  reluctant|reluctantly|reluctanly|reluctants|mo...   \n",
              "18062226  reluctant|reluctantly|reluctanly|reluctants|mo...   \n",
              "18259482  reluctant|reluctantly|reluctanly|reluctants|mo...   \n",
              "\n",
              "                          matched_term  \n",
              "12929593                   \\babusers\\b  \n",
              "12929712                   \\babusers\\b  \n",
              "12929975                   \\babusers\\b  \n",
              "12930089                   \\babusers\\b  \n",
              "5774027   \\bnon\\-alcoholic/alcoholic\\b  \n",
              "...                                ...  \n",
              "18023216                 \\bunwilling\\b  \n",
              "18060104                 \\bunwilling\\b  \n",
              "18061939                 \\bunwilling\\b  \n",
              "18062226                 \\bunwilling\\b  \n",
              "18259482                 \\bunwilling\\b  \n",
              "\n",
              "[20045 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-7a9809fa-2d36-4d5b-a294-70c4f5405b34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence ID</th>\n",
              "      <th>ROW_ID_x</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>CHARTDATE</th>\n",
              "      <th>...</th>\n",
              "      <th>DOD_SSN</th>\n",
              "      <th>EXPIRE_FLAG</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>regex</th>\n",
              "      <th>matched_term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12929593</th>\n",
              "      <td>12929593</td>\n",
              "      <td>48563</td>\n",
              "      <td>13855</td>\n",
              "      <td>107256.0</td>\n",
              "      <td>2137-05-06</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Per OMR, reports that his parents were substan...</td>\n",
              "      <td>abuser|ivdabuser|abuser's|abused\\-abuser|abuse...</td>\n",
              "      <td>\\babusers\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12929712</th>\n",
              "      <td>12929712</td>\n",
              "      <td>371212</td>\n",
              "      <td>13855</td>\n",
              "      <td>107256.0</td>\n",
              "      <td>2137-05-01</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Per OMR, reports that his parents were substan...</td>\n",
              "      <td>abuser|ivdabuser|abuser's|abused\\-abuser|abuse...</td>\n",
              "      <td>\\babusers\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12929975</th>\n",
              "      <td>12929975</td>\n",
              "      <td>48561</td>\n",
              "      <td>13855</td>\n",
              "      <td>125990.0</td>\n",
              "      <td>2135-07-06</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Reports that his parents were substance abuse...</td>\n",
              "      <td>abuser|ivdabuser|abuser's|abused\\-abuser|abuse...</td>\n",
              "      <td>\\babusers\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12930089</th>\n",
              "      <td>12930089</td>\n",
              "      <td>48562</td>\n",
              "      <td>13855</td>\n",
              "      <td>108743.0</td>\n",
              "      <td>2135-12-19</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Per OMR, reports that his parents were substan...</td>\n",
              "      <td>abuser|ivdabuser|abuser's|abused\\-abuser|abuse...</td>\n",
              "      <td>\\babusers\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5774027</th>\n",
              "      <td>5774027</td>\n",
              "      <td>22009</td>\n",
              "      <td>5760</td>\n",
              "      <td>169133.0</td>\n",
              "      <td>2109-01-22</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Her hepatic workup was consistent with non-alc...</td>\n",
              "      <td>alcoholic|non\\-alcoholic/alcoholic|prealcoholi...</td>\n",
              "      <td>\\bnon\\-alcoholic/alcoholic\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18023216</th>\n",
              "      <td>18023216</td>\n",
              "      <td>1313078</td>\n",
              "      <td>3895</td>\n",
              "      <td>151195.0</td>\n",
              "      <td>2140-04-03</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Lungs diminished at bases, clear bil upper lo...</td>\n",
              "      <td>reluctant|reluctantly|reluctanly|reluctants|mo...</td>\n",
              "      <td>\\bunwilling\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18060104</th>\n",
              "      <td>18060104</td>\n",
              "      <td>584615</td>\n",
              "      <td>58757</td>\n",
              "      <td>130745.0</td>\n",
              "      <td>2140-06-15</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td># STROKE - s/p R sided CVA in '[**33**] with ...</td>\n",
              "      <td>reluctant|reluctantly|reluctanly|reluctants|mo...</td>\n",
              "      <td>\\bunwilling\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18061939</th>\n",
              "      <td>18061939</td>\n",
              "      <td>52008</td>\n",
              "      <td>58757</td>\n",
              "      <td>130745.0</td>\n",
              "      <td>2140-06-19</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td># STROKE - s/p R sided CVA in '[**33**] with ...</td>\n",
              "      <td>reluctant|reluctantly|reluctanly|reluctants|mo...</td>\n",
              "      <td>\\bunwilling\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18062226</th>\n",
              "      <td>18062226</td>\n",
              "      <td>42197</td>\n",
              "      <td>67418</td>\n",
              "      <td>161948.0</td>\n",
              "      <td>2189-10-12</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>He had a full work up at that time, according ...</td>\n",
              "      <td>reluctant|reluctantly|reluctanly|reluctants|mo...</td>\n",
              "      <td>\\bunwilling\\b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18259482</th>\n",
              "      <td>18259482</td>\n",
              "      <td>1469313</td>\n",
              "      <td>15904</td>\n",
              "      <td>125754.0</td>\n",
              "      <td>2155-09-13</td>\n",
              "      <td>...</td>\n",
              "      <td>2155-10-30</td>\n",
              "      <td>1</td>\n",
              "      <td>was initially unwilling to be turned but is mu...</td>\n",
              "      <td>reluctant|reluctantly|reluctanly|reluctants|mo...</td>\n",
              "      <td>\\bunwilling\\b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20045 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a9809fa-2d36-4d5b-a294-70c4f5405b34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-0c7aaa43-3589-427c-a3f1-34febed3b6a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c7aaa43-3589-427c-a3f1-34febed3b6a1')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-0c7aaa43-3589-427c-a3f1-34febed3b6a1 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a9809fa-2d36-4d5b-a294-70c4f5405b34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a9809fa-2d36-4d5b-a294-70c4f5405b34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "doubt_word_list = pd.read_csv(\"stigma_word_list_round_3_gpt.csv\")\n",
        "#Removed difficult due to high noise\n",
        "words = ['difficult', 'suspicious','aggressive','unstable']\n",
        "\n",
        "# Trim leading and trailing whitespaces\n",
        "doubt_word_list['similar_word'] = doubt_word_list['similar_word'].str.strip()\n",
        "\n",
        "# Filter the dataframe of the words which were found to have high amount of noise\n",
        "doubt_word_list_pruned = doubt_word_list[~doubt_word_list['similar_word'].isin(words)]\n",
        "doubt_word_list_pruned\n",
        "\n",
        "\n",
        "#Strip whitespace of list of words in doubt_word_list\n",
        "doubt_word_list_pruned\n",
        "# Create regex which is each stem word + all similar_words identified in the word embeddings step, which will be used to search the charts\n",
        "def group_and_create_regex(dataframe):\n",
        "    grouped_data = dataframe.groupby('stem_word')['similar_word'].apply(list).reset_index()\n",
        "    grouped_data['regex'] = grouped_data.apply(lambda row: '|'.join([re.escape(row['stem_word'])] + [re.escape(word) for word in row['similar_word']]), axis=1)\n",
        "    return grouped_data\n",
        "\n",
        "grouped_data = group_and_create_regex(doubt_word_list_pruned)\n",
        "print(grouped_data)\n",
        "\n",
        "# Create function that takes regex from each row and searches the charts\n",
        "def identify_matching_strings(full_dataframe, grouped_data):\n",
        "    matching_dataframes = []\n",
        "\n",
        "    for index, row in grouped_data.iterrows():\n",
        "        regex = row['regex']\n",
        "        terms = row['similar_word']\n",
        "        for term in terms:\n",
        "            term = r\"\\b\" + re.escape(term) + r\"\\b\"  # Adding word boundaries\n",
        "            matches = full_dataframe.loc[full_dataframe['Sentence'].str.contains(term, flags=re.IGNORECASE, na=False)].copy()\n",
        "            matches['regex'] = regex\n",
        "            matches['matched_term'] = term  # Add the matched term as a new column\n",
        "            matching_dataframes.append(matches)\n",
        "\n",
        "    first_dataframe = pd.concat(matching_dataframes)\n",
        "    print(f\"Size before patient related filter: {first_dataframe.shape[0]}\")  # Print the size before filter\n",
        "\n",
        "    final_dataframe = first_dataframe.loc[first_dataframe['Sentence'].str.contains(r'\\b(?:pt|patient|pateint)\\b', flags=re.IGNORECASE, na=False)].copy()  # Add condition here\n",
        "\n",
        "    print(f\"Final size after patient related filter: {final_dataframe.shape[0]}\")  # Print the size after filter\n",
        "\n",
        "    return first_dataframe\n",
        "\n",
        "\n",
        "\n",
        "small_dataframe = full_dataframe.head(1000000)\n",
        "\n",
        "\n",
        "result = identify_matching_strings(full_dataframe, grouped_data)\n",
        "result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpWPRoHKqHbS"
      },
      "source": [
        "# Full matching dataset for stigmatizing labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mpl6x53rTZfl"
      },
      "outputs": [],
      "source": [
        "\n",
        "quoted_dataframe = result\n",
        "\n",
        "quoted_dataframe[\"stigmatizing_patient_label\"] = \"\"\n",
        "quoted_dataframe[\"annotator_comments\"] = \"\"\n",
        "# X matching sentences from total of 9,118,501 sentences from 366,002 charts\n",
        "# input code showing length of quoted_dataframe vs quoted_dataframe2\n",
        "quoted_dataframe2 = quoted_dataframe.drop_duplicates(subset=['Sentence'])\n",
        "quoted_dataframe2.to_csv(\"stigmatizing_labels_dataframe_annotate.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TclhgJVkqMgu"
      },
      "source": [
        "# Creating Annotation Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsC2y8__qE1Y",
        "outputId": "e8afe101-eff4-498d-bb13-8f34c80f6fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/2_Annotation\n"
          ]
        }
      ],
      "source": [
        "# Create the annotation samples\n",
        "quoted_dataframe2 = pd.read_csv(\"stigmatizing_labels_dataframe_annotate.csv\")\n",
        "np.random.seed(23)\n",
        "# List of tuples (sample size, dataframe name)\n",
        "sample_params = [(50, 'Drew_initial'), (5, 'cocode'), (100, 'reliability'), (400, 'annotation_Annie'), (500, 'annotation_Drew')]\n",
        "\n",
        "# Dictionary to hold the dataframes\n",
        "dfs = {}\n",
        "\n",
        "for size, name in sample_params:\n",
        "    # Random sample without replacement\n",
        "    sample_df = quoted_dataframe2.sample(n=size, replace=False)\n",
        "\n",
        "    # Add the sample to the dictionary\n",
        "    dfs[name] = sample_df\n",
        "\n",
        "    # Remove the sampled rows from the original dataframe\n",
        "    quoted_dataframe2 = quoted_dataframe2.drop(sample_df.index)\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Stigmatizing Labels/2_Annotation\n",
        "\n",
        "# Now you can access each dataframe using their names.\n",
        "# For example, to access the 'Drew' dataframe:\n",
        "drew_df = dfs['Drew_initial']\n",
        "drew_df\n",
        "drew_df.to_excel(\"Drew_initial_sample_stigmatizing_labels.xlsx\", index = False)\n",
        "\n",
        "cocode_df = dfs['cocode']\n",
        "cocode_df\n",
        "cocode_df.to_excel(\"cocode_sample_stigmatizing_labels.xlsx\", index = False)\n",
        "\n",
        "reliability_df = dfs['reliability']\n",
        "reliability_df\n",
        "reliability_df.to_excel(\"reliability_sample_stigmatizing_labels.xlsx\", index = False)\n",
        "\n",
        "annotation_Annie_df = dfs['annotation_Annie']\n",
        "annotation_Annie_df\n",
        "annotation_Annie_df.to_excel(\"annotation_Annie_sample_stigmatizing_labels.xlsx\", index = False)\n",
        "\n",
        "annotation_Drew_df = dfs['annotation_Drew']\n",
        "annotation_Drew_df\n",
        "annotation_Drew_df.to_excel(\"annotation_Drew_sample_stigmatizing_labels.xlsx\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrdxgmAxmzhr"
      },
      "source": [
        "Explore the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gg1A9AkKpJM"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNZjEB7eepjM4VKoBp4rw60",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}