{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyME08ALBjIVWk0SYZ+NbDhw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drew-walkerr/Diss_Detecting_Provider_Bias/blob/main/doubt_markers_lexicon_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n"
      ],
      "metadata": {
        "id": "DFIY5gxgoVCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFX50pdp738q",
        "outputId": "f63eb85e-11f8-4792-9bfb-91d27d362ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import Cython\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from collections import defaultdict\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import Levenshtein, re\n",
        "import sys\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Doubt Markers/1_Data Prep\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hihS408RC0xC",
        "outputId": "0753d41f-37ee-414d-a71d-e56ca713f555"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/Doubt Markers/1_Data Prep\n",
            "1_doubt_makers_word_embeddings.py\n",
            "doubt_markers_lexicon_dev.ipynb\n",
            "doubt_words_lexicon_stem_and_similar_round1.csv\n",
            "trig-vectors-phrase.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading word2vec model\n",
        "\n",
        "We're using BioWordVec, trained on MIMIC-III data: \n",
        "\n",
        "1. Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n",
        "2. Chen Q, Peng Y, Lu Z. BioSentVec: creating sentence embeddings for biomedical texts. The 7th IEEE International Conference on Healthcare Informatics. 2019.\n",
        "\n"
      ],
      "metadata": {
        "id": "L0VCU7pt8NWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Diss_Detecting_Provider_Bias/Aim 1/BioWordVec_PubMed_MIMICIII_d200.vec.bin', binary = True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zXyxskB8KrW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doubt Markers \n",
        "\n",
        "\n",
        "# Beach et al. 2021 Testimonial Injustice : \n",
        "* Judgment words: \"adamant\", \"apparently\", \"claims\",\"insists\", \"states\", \n",
        "\n",
        "# Biber and Finegan, 1989 Styles of stance in english \n",
        "* Doubt adverbs: allegedly, apparently, conceivably, ostensibly, perchance, perhaps, possibly, presumably, purportedly, reportedly, reputedly, seemingly, supposedly\n",
        "* Doubt verbs: disbelieve, doubt, suspected, speculated, \n",
        "* Doubt adjectives: alleged, arguable, conceivable, disputable, doubtful, dubious, imaginable, improbable, presumable, questionable, reputed, supposed, uncertain, likely  \n",
        "\n"
      ],
      "metadata": {
        "id": "x8in2sZC8SUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Doubt Markers/Evidentials identified through:\n",
        "# Beach et al. 2021 Testimonial Injustice : \n",
        "# Judgment words: \"adamant\", \"apparently\", \"claims\",\"insists\", \"states\", \n",
        "\n",
        "## List: \n",
        "# Biber and Finegan, 1989 Styles of stance in english \n",
        "## Doubt adverbs: allegedly, apparently, conceivably, ostensibly, perchance, perhaps, possibly, presumably, purportedly, reportedly, reputedly, seemingly, supposedly\n",
        "## Doubt verbs: disbelieve, doubt, suspected, speculated, \n",
        "## Doubt adjectives: alleged, arguable, conceivable, disputable, doubtful, dubious, imaginable, improbable, presumable, questionable, reputed, supposed, uncertain, likely  \n",
        "\n",
        "\n",
        "# Papafragou et al., 2007, Evidentiality in language and cognition\n",
        "## Not using:  Evidentials: complains, complained, complaining, denies, denied, denying, endorses, endorsed, endorsing, notes, noting , reports, reporting, says, saying, tells, told, telling \n",
        "\n",
        "\n",
        "doubt_stem_words = [\"adamant\", \"claims\", \"insists\", \"states\", \"allegedly\", \"apparently\", \"conceivably\", \"ostensibly\", \"perchance\", \"perhaps\", \"possibly\", \"presumably\", \"purportedly\", \"reportedly\", \"reputedly\", \"seemingly\", \"supposedly\", \"disbelieve\", \"doubt\", \"suspected\", \"speculated\", \"alleged\", \"arguable\", \"conceivable\", \"disputable\", \"doubtful\", \"dubious\", \"imaginable\", \"improbable\", \"presumable\", \"questionable\", \"reputed\", \"supposed\", \"uncertain\", \"likely\"]\n",
        "\n",
        "\n",
        "\n",
        "bias_words_df = pd.DataFrame({\n",
        "    'stem_word': doubt_stem_words\n",
        "})"
      ],
      "metadata": {
        "id": "0ZoYHIJf8Q0p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "bias_words_df['most_similar_words'] = bias_words_df['stem_word'].apply(model.most_similar)\n",
        "\n",
        "bias_words_df_2 = bias_words_df.explode(\"most_similar_words\", ignore_index=True)\n",
        "bias_words_df_2['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n",
        "# bias_words_df_2[['similar_word','similarity_score']] =\n",
        "words_sep = pd.DataFrame(bias_words_df_2['most_similar_words'].values.tolist())\n",
        "words_sep['new_word_id'] = range(1, 1 + len(bias_words_df_2))\n",
        "bias_words_3 = bias_words_df_2.merge(words_sep, on = 'new_word_id')\n",
        "#bias_words_3['similar_word'], bias_words_3['score'] = bias_words_3[3],bias_words_3[4]\n",
        "\n",
        "bias_words_3= bias_words_3.rename(columns={0: \"similar_word\", 1: \"score\"})\n",
        "bias_words_3[\"Relevant_to_study\"] = \"\"\n",
        "bias_words_3.to_csv(\"doubt_words_lexicon_stem_and_similar_round1.csv\")\n",
        "\n",
        "bias_words_3"
      ],
      "metadata": {
        "id": "6QKm74hL-TG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "910efee2-0297-479d-e804-8ca814969c1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    stem_word                         most_similar_words  new_word_id  \\\n",
              "0     adamant            (adamantly, 0.8700840473175049)            1   \n",
              "1     adamant             (adamants, 0.8528293967247009)            2   \n",
              "2     adamant               (damant, 0.8377524614334106)            3   \n",
              "3     adamant           (adamanteous, 0.821442723274231)            4   \n",
              "4     adamant  (adamant/belligerant, 0.8073655962944031)            5   \n",
              "..        ...                                        ...          ...   \n",
              "345    likely             (islikely, 0.7951004505157471)          346   \n",
              "346    likely             (probably, 0.7948128581047058)          347   \n",
              "347    likely              (likelyt, 0.7872053384780884)          348   \n",
              "348    likely      (possibly/likely, 0.7838917374610901)          349   \n",
              "349    likely        (likely/likely, 0.7824686169624329)          350   \n",
              "\n",
              "            similar_word     score Relevant_to_study  \n",
              "0              adamantly  0.870084                    \n",
              "1               adamants  0.852829                    \n",
              "2                 damant  0.837752                    \n",
              "3            adamanteous  0.821443                    \n",
              "4    adamant/belligerant  0.807366                    \n",
              "..                   ...       ...               ...  \n",
              "345             islikely  0.795100                    \n",
              "346             probably  0.794813                    \n",
              "347              likelyt  0.787205                    \n",
              "348      possibly/likely  0.783892                    \n",
              "349        likely/likely  0.782469                    \n",
              "\n",
              "[350 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff54cad5-c781-4d43-843d-47e6d419a4df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stem_word</th>\n",
              "      <th>most_similar_words</th>\n",
              "      <th>new_word_id</th>\n",
              "      <th>similar_word</th>\n",
              "      <th>score</th>\n",
              "      <th>Relevant_to_study</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>adamant</td>\n",
              "      <td>(adamantly, 0.8700840473175049)</td>\n",
              "      <td>1</td>\n",
              "      <td>adamantly</td>\n",
              "      <td>0.870084</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adamant</td>\n",
              "      <td>(adamants, 0.8528293967247009)</td>\n",
              "      <td>2</td>\n",
              "      <td>adamants</td>\n",
              "      <td>0.852829</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adamant</td>\n",
              "      <td>(damant, 0.8377524614334106)</td>\n",
              "      <td>3</td>\n",
              "      <td>damant</td>\n",
              "      <td>0.837752</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adamant</td>\n",
              "      <td>(adamanteous, 0.821442723274231)</td>\n",
              "      <td>4</td>\n",
              "      <td>adamanteous</td>\n",
              "      <td>0.821443</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>adamant</td>\n",
              "      <td>(adamant/belligerant, 0.8073655962944031)</td>\n",
              "      <td>5</td>\n",
              "      <td>adamant/belligerant</td>\n",
              "      <td>0.807366</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>likely</td>\n",
              "      <td>(islikely, 0.7951004505157471)</td>\n",
              "      <td>346</td>\n",
              "      <td>islikely</td>\n",
              "      <td>0.795100</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>likely</td>\n",
              "      <td>(probably, 0.7948128581047058)</td>\n",
              "      <td>347</td>\n",
              "      <td>probably</td>\n",
              "      <td>0.794813</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>likely</td>\n",
              "      <td>(likelyt, 0.7872053384780884)</td>\n",
              "      <td>348</td>\n",
              "      <td>likelyt</td>\n",
              "      <td>0.787205</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>likely</td>\n",
              "      <td>(possibly/likely, 0.7838917374610901)</td>\n",
              "      <td>349</td>\n",
              "      <td>possibly/likely</td>\n",
              "      <td>0.783892</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>likely</td>\n",
              "      <td>(likely/likely, 0.7824686169624329)</td>\n",
              "      <td>350</td>\n",
              "      <td>likely/likely</td>\n",
              "      <td>0.782469</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>350 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff54cad5-c781-4d43-843d-47e6d419a4df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff54cad5-c781-4d43-843d-47e6d419a4df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff54cad5-c781-4d43-843d-47e6d419a4df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following 10 most similar words identified, we'll use  stigmatizing_labels_descriptors_lexicon_stem_and_similar_round1.csv to filter out words deemed irrelevant to study by JL and DW. "
      ],
      "metadata": {
        "id": "sIrhzlVq-Y1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misspelling Generator\n",
        "* "
      ],
      "metadata": {
        "id": "4RATkUDy-Nx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Misspelling Generator\n",
        "\n",
        "\n",
        "\n",
        "def generate_spelling_variants(seedwordlist, word_vectors, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1):\n",
        "    \"\"\"\n",
        "        setting -> 0 = weighted levenshtein ratios\n",
        "                -> 1 = standard levenshtein ratios\n",
        "\n",
        "    :param seedwordlist:            list of words for which spelling variants are to be generated\n",
        "    :param word_vectors:            the word vector model\n",
        "    :param semantic_search_length:  the number of semantically similar terms to include in each iteration\n",
        "    :param levenshtein_threshold:   the threshold for levenshtein ratio\n",
        "\n",
        "    :return: dictionary containing the seedwords as key and all the variants as a list of values\n",
        "\n",
        "    \"\"\"\n",
        "    vars = defaultdict(list)\n",
        "    for seedword in seedwordlist:\n",
        "        #a dictionary to hold all the variants, key: the seedword, value: the list of possible misspellings\n",
        "        #a dynamic list of terms that are still to be expanded\n",
        "        terms_to_expand = []\n",
        "        terms_to_expand.append(seedword)\n",
        "        all_expanded_terms = []\n",
        "        level = 1\n",
        "        while len(terms_to_expand)>0:\n",
        "                t = terms_to_expand.pop(0)\n",
        "                all_expanded_terms.append(t)\n",
        "                try:\n",
        "                    similars = word_vectors.most_similar(t, topn=semantic_search_length)\n",
        "                    for similar in similars:\n",
        "                        similar_term = similar[0]\n",
        "                        if setting == 1:\n",
        "                            seq_score = Levenshtein.ratio(str(similar_term),seedword)\n",
        "                        if setting == 0:\n",
        "                            seq_score = weighted_levenshtein_ratio(str(similar_term), seedword)\n",
        "                        if seq_score>levenshtein_threshold:\n",
        "                            if not re.search(r'\\_',similar_term):\n",
        "                                vars[seedword].append(similar_term)\n",
        "                                if not similar_term in all_expanded_terms and not similar_term in terms_to_expand:\n",
        "                                    terms_to_expand.append(similar_term)\n",
        "                except:\n",
        "                        pass\n",
        "                level+=1\n",
        "        vars[seedword] = list(set(vars[seedword]))\n",
        "    return vars\n",
        "\n",
        "bias_stem_words_round_2 = pd.read_csv(\"word_list_round_2.csv\")\n",
        "bias_stem_words_round_2[\"similar_word\"] = bias_stem_words_round_2[\"similar_word\"].replace(\"_\", \" \", regex = True)\n",
        "\n",
        "bias_expanded_word_list = bias_stem_words_round_2[\"similar_word\"]\n",
        "\n",
        "\n",
        "expanded = generate_spelling_variants(bias_expanded_word_list, model2, semantic_search_length=500, levenshtein_threshold = 0.85, setting = 1)\n",
        "\n",
        "df = pd.DataFrame.from_dict(expanded, orient ='index')\n",
        "\n",
        "\n",
        "\n",
        "df.to_csv(\"expanded_misspellings.csv\")"
      ],
      "metadata": {
        "id": "fediWTrj-Mro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "af05ce5e-6f70-4a80-ebac-942f1432d847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dc718368f488>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mbias_stem_words_round_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_list_round_2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mbias_stem_words_round_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"similar_word\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_stem_words_round_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"similar_word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word_list_round_2.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Git Commands"
      ],
      "metadata": {
        "id": "FZXYiQtUdsxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git config --global user.email \"andrew.walker@emory.edu\"\n",
        "#!git config --global user.name \"drew-walkerr\"\n",
        "#!git add \"doubt_markers_lexicon_dev.ipynb\"\n",
        "\n",
        "#!git commit -m \"created first prototype for doubt markers word embeddings expansion\"\n",
        "#!git push https://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@github.com/drew-walkerr/Diss_Detecting_Provider_Bias.git"
      ],
      "metadata": {
        "id": "SvVn72cIZ4Je"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}